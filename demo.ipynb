{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ffbdd-1b1e-4601-88c3-7744df0acaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb\n",
    "%pip install openai\n",
    "%pip install matplotlib\n",
    "%pip install open-clip-torch\n",
    "%pip install langchain-text-splitters\n",
    "%pip install -U langchain-community pypdf\n",
    "%pip install torch\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4081303",
   "metadata": {},
   "source": [
    "## Image Similarity Search using CLIP Model\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import os\n",
    "import torch\n",
    "\n",
    "CHROMA_PATH='./chroma_db'\n",
    "DATA_PATH='' # images path\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "multimodal_clip_collection = chroma_client.get_or_create_collection(\n",
    "    name='multimodal',\n",
    "    embedding_function=OpenCLIPEmbeddingFunction(device=device),\n",
    "    data_loader=ImageLoader()\n",
    ")\n",
    "\n",
    "\n",
    "image_paths = os.listdir(DATA_PATH)\n",
    "\n",
    "multimodal_clip_collection.add(\n",
    "    ids=[str(id) for id in range(len(image_paths))],\n",
    "    uris=[os.path.join(DATA_PATH, p) for p in image_paths]\n",
    ")\n",
    "\n",
    "print(multimodal_clip_collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435b7c4",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_results(queries, query_res):\n",
    "    def image_display(uri):\n",
    "        return f'<img src=\"{uri}\"/>'\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        display(HTML(f\"<h3>Results for '{query}'</h3>\"))\n",
    "        df = pd.DataFrame(reversed([\n",
    "            {\n",
    "                \"Id\": id,\n",
    "                \"Distance\": dist,\n",
    "                \"URI\": uri,\n",
    "                \"Image\": uri\n",
    "            }\n",
    "            for id, dist, uri in zip(\n",
    "                query_res['ids'][i],\n",
    "                query_res['distances'][i],\n",
    "                query_res['uris'][i]\n",
    "            )\n",
    "        ]))\n",
    "        with pd.option_context('display.max_colwidth', None):\n",
    "            display(HTML(df.to_html(escape=False, formatters={'Image': image_display})))\n",
    "\n",
    "# List of queries to try\n",
    "query_texts = []\n",
    "\n",
    "# Execute the queries\n",
    "query_res = multimodal_clip_collection.query(\n",
    "    query_texts=query_texts,\n",
    "    n_results=5,\n",
    "    include=['distances', 'data', 'uris']\n",
    ")\n",
    "\n",
    "# And output the results\n",
    "print_results(query_texts, query_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec15feb-8283-4248-8d4a-b5d7aa2da7f8",
   "metadata": {},
   "source": [
    "## Implement a Retrieval Augmented Generation (RAG) Pipeline with LangChain, ChromaDB, and OpenAI\n",
    "![image info](./RAG_diagram.svg.png)\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85ddc8",
   "metadata": {},
   "source": [
    "### Chunk document with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada43d63-c8bd-4f56-b309-dc7bd3b48e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "DATA_PATH='' # path to documents\n",
    "\n",
    "def load_and_chunk_pdfs(folder_path):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks_all_docs = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            path = os.path.join(folder_path, filename)\n",
    "\n",
    "            loader = PyPDFLoader(path)\n",
    "            pages = loader.load()\n",
    "            chunks = splitter.split_documents(pages)\n",
    "            chunks_all_docs += chunks\n",
    "\n",
    "    return chunks_all_docs\n",
    "\n",
    "# \n",
    "chunks = load_and_chunk_pdfs(DATA_PATH)\n",
    "\n",
    "# Total chunks, first and last chunk data\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(chunks[0].metadata)\n",
    "print(chunks[-1].metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a035f5d",
   "metadata": {},
   "source": [
    "### Create Embeddings and store in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "OPENAI_KEY=os.environ['OPENAI_KEY']\n",
    "openai_client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "# Post request to openai, use embedding model\n",
    "BATCH_SIZE=50\n",
    "embeddings = []\n",
    "documents = [chunk.page_content for chunk in chunks]\n",
    "for i in range(0, len(documents), BATCH_SIZE):\n",
    "    batch = documents[i:i+BATCH_SIZE]\n",
    "    \n",
    "    resp = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=batch\n",
    "    )\n",
    "    embeddings += [r.embedding for r in resp.data]\n",
    "    time.sleep(0.05)\n",
    "\n",
    "\n",
    "metadatas = [chunk.metadata for chunk in chunks]\n",
    "ids = [str(id) for id in range(len(chunks))]\n",
    "\n",
    "rag_collection = chroma_client.get_or_create_collection('pdf-rag')\n",
    "\n",
    "    \n",
    "rag_collection.add(\n",
    "    ids=ids,\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c9f0",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query text to try\n",
    "query_text = ''\n",
    "\n",
    "def get_query_embedding(query_text): \n",
    "    return openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=query_text\n",
    "    ).data[0].embedding\n",
    "\n",
    "results = rag_collection.query(\n",
    "    query_embeddings=[get_query_embedding(query_text)],\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "retrieved_texts = [doc for doc in results['documents'][0]]\n",
    "retrieved_metadatas = [md for md in results['metadatas'][0]]\n",
    "\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Source: {md['source']}\\n Page: {md['page']}\\nText: {text}\"\n",
    "    for text, md in zip(retrieved_texts, retrieved_metadatas)\n",
    ")\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Answer this question using only the following context:\\n{context}\\nQuestion: {query_text}\"}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
